{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07c81bff",
      "metadata": {
        "id": "07c81bff"
      },
      "source": [
        "# Garage Data System: Dual PySpark & SQL Analysis\n",
        "\n",
        "**Contents**  \n",
        "- [Q.1](#q1)\n",
        "- [Q.2](#q2)\n",
        "- [Q.3](#q3)\n",
        "- [Q.4](#q4)\n",
        "- [Q.5](#q5)\n",
        "- [Q.6](#q6)\n",
        "- [Q.7](#q7)\n",
        "- [Q.8](#q8)\n",
        "- [Q.9](#q9)\n",
        "- [Q.10](#q10)\n",
        "- [Q.11](#q11)\n",
        "- [Q.12](#q12)\n",
        "- [Q.13](#q13)\n",
        "- [Q.14](#q14)\n",
        "- [Q.15](#q15)\n",
        "- [Q.16](#q16)\n",
        "- [Q.17](#q17)\n",
        "- [Q.18](#q18)\n",
        "- [Q.19](#q19)\n",
        "- [Q.20](#q20)\n",
        "- [Q.21](#q21)\n",
        "- [Q.22](#q22)\n",
        "- [Q.23](#q23)\n",
        "- [Q.24](#q24)\n",
        "- [Q.25](#q25)\n",
        "- [Q.26](#q26)\n",
        "- [Q.27](#q27)\n",
        "- [Q.28](#q28)\n",
        "- [Q.29](#q29)\n",
        "- [Q.30](#q30)\n",
        "- [Q.31](#q31)\n",
        "- [Q.32](#q32)\n",
        "- [Q.33](#q33)\n",
        "- [Q.34](#q34)\n",
        "- [Q.35](#q35)\n",
        "- [Q.36](#q36)\n",
        "- [Q.37](#q37)\n",
        "- [Q.38](#q38)\n",
        "- [Q.39](#q39)\n",
        "- [Q.40](#q40)\n",
        "- [Q.41](#q41)\n",
        "- [Q.42](#q42)\n",
        "- [Q.43](#q43)\n",
        "- [Q.44](#q44)\n",
        "- [Q.45](#q45)\n",
        "- [Q.46](#q46)\n",
        "- [Q.47](#q47)\n",
        "- [Q.48](#q48)\n",
        "- [Q.49](#q49)\n",
        "- [Q.50](#q50)\n",
        "- [Q.51](#q51)\n",
        "- [Q.52](#q52)\n",
        "- [Q.53](#q53)\n",
        "- [Q.54](#q54)\n",
        "- [Q.55](#q55)\n",
        "- [Q.56](#q56)\n",
        "- [Q.57](#q57)\n",
        "- [Q.58](#q58)\n",
        "- [Q.59](#q59)\n",
        "- [Q.60](#q60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda4bcd7",
      "metadata": {
        "id": "eda4bcd7"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install necessary libraries\n",
        "!pip install pyspark num2words --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/pavanhalde21/PySpark_lecture_files/blob/main/Questions_01/garage_analysis_sql_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "CoArCEfCk5Ft"
      },
      "id": "CoArCEfCk5Ft"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47db198",
      "metadata": {
        "id": "b47db198"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 2: Create dummy CSV files for the analysis\n",
        "%%writefile customer_table.csv\n",
        "cid,cname,cadd,contact,creditdays,date,gender\n",
        "1001,\"cyona blake\",NEW YORK,12345684,20,20-JAN-11,female\n",
        "1002,\"JOHN SMITH\",NEW JERSI,1341684,20,21-FEB-11,male\n",
        "1003,\"JORDEN WOOD\",PRAG,1805184,20,22-MAR-11,male\n",
        "1004,CHRISTANA,MANHATTON,1125684,31,23-APR-13,female\n",
        "1005,\"TOM HILL\",LONDON,1239284,10,25-JUN-15,male\n",
        "1006,\"KAMILA JOSEF\",PRAISE,124568,9,28-JUL-11,female\n",
        "1007,\"ANDRU SYMON\",TEXAS,125654,15,01-APR-16,male\n",
        "1008,\"SANJU SAMSUNG\",NEW YORK,12346341,4,20-JAN-16,male\n",
        "\n",
        "%%writefile ser_det_table.csv\n",
        "sid,cid,eid,spid,type_veh,veh_no,typ_ser,ser_date,qty,sp_rate,sp_amt,sp_g,ser_amt,comm,total\n",
        "6001,1001,3001,4001,TWO WHEELER,MH15CA3228,TUBE DAMAGED,02-JAN-11,1.0,250,250,35,50,0,335\n",
        "6002,1002,3002,4002,TWO WHEELER,MH16U5713,FULL SERVICING,04-MAR-11,1.0,400,400,52,300,50,752\n",
        "6003,1003,3004,4005,TWO WHEELER,MH12PQ1313,CLUTCH WIRE,22-AUG-11,1.0,129,129,0,10,0,139\n",
        "6004,1002,3002,4002,TWO WHEELER,MH16U5713,FULL SERVICING,05-MAY-11,1.0,400,400,52,300,50,752\n",
        "6005,1004,3001,4009,TWO WHEELER,MH14PA335,COLOR,21-OCT-11,2.5,340,850,119,500,150,1469\n",
        "6006,1006,3001,4009,TWO WHEELER,MH12WE334,COLOR,01-DEC-11,2.5,340,850,119,500,150,1469\n",
        "6007,1007,3001,4001,FOUR WHEELER,MH17BB1345,TUBE DAMAGED,01-JAN-12,1.0,250,250,35,150,0,435\n",
        "\n",
        "%%writefile employee_table.csv\n",
        "eid,ename,ejob,eadd,econtact,esal,edoj,edol\n",
        "3001,\"STEVEN KING\",PAINTER,NEW YORK,10367454,1200,01-JAN-08,\n",
        "3002,\"DAVID AUSTIN\",FITTER,MANAHTON,10367434,1100,19-AUG-10,\n",
        "3003,\"BRUCE ERENST\",MECHANIC,NEW JERCY,10367264,2200,08-SEP-10,\n",
        "3004,\"LUIS POP\",MECHANIC,NEW JERCY,10367264,1700,19-OCT-19,06-SEP-10\n",
        "3005,\"SHERI GOMES\",FITTER,PARIS,10327264,1000,19-OCT-09,01-AUG-10\n",
        "3000,\"JAMES PHILIP\",FITTER,PARIS,10322264,,01-JAN-08,\n",
        "\n",
        "%%writefile sparepart_table.csv\n",
        "spid,spname,sprate,spunit\n",
        "4001,TWO WHEELER TUBE,250,NOS\n",
        "4002,TWO WHEELER ENGINE OIL,400,LTRS\n",
        "4003,FOUR WHEELER ENGINE OIL,5000,LTRS\n",
        "4004,TWO WHEELER CARBORATOR,680,NOS\n",
        "4005,TWO WHEELER CLUTCH WIRE,129,MTRS\n",
        "4006,TWO WHEELER TAIIL LIGHT,100,NOS\n",
        "4007,TWO WHEELER INDICATORS,150,NOS\n",
        "4008,FOUR WHEELER GASKIT,1340,NOS\n",
        "4009,WHITE COLOUR,340,LTRS\n",
        "4010,BLACK COLOUR,240,LTRS\n",
        "4011,TWO WHEELER SIDE MIRROR,250,NOS\n",
        "4012,FOUR WHEELER SIDE MIRROR,450,NOS\n",
        "4013,TWO WHEELER SHOCKUP,1320,PAIR\n",
        "4014,FOUR WHEELER BUMPER,6000,NOS\n",
        "4015,FOUR WHEELER FRONT GLASS,7000,PCS\n",
        "\n",
        "%%writefile purchase_table.csv\n",
        "pid,vid,spid,pqty,sprate,spgst,pdate,transcost,total,rcv_eid\n",
        "5001,2001,4001,10,250,350.0,01-JAN-11,1300,4150.0,3001\n",
        "5002,2002,4002,4,400,288.0,02-MAR-11,100,1988.0,3001\n",
        "5003,2003,4004,8,680,761.6,12-JUN-11,50,6251.6,3003\n",
        "5004,2004,4005,10,129,154.8,22-AUG-11,125,1569.8,3004\n",
        "5005,2005,4006,20,100,300.0,07-SEP-11,20,2320.0,3003\n",
        "5006,2006,4007,30,150,630.0,11-OCT-11,60,5190.0,3000\n",
        "5007,2001,4003,20,5000,14000.0,07-SEP-11,1000,115000.0,3000\n",
        "5008,2006,4005,1,129,15.48,12-OCT-11,50,194.48,3005\n",
        "5009,2006,4005,1,129,15.48,15-OCT-11,50,194.48,3005\n",
        "5010,2006,4009,5,340,238.0,20-OCT-11,0,1938.0,3005\n",
        "\n",
        "%%writefile vendor_table.csv\n",
        "vid,vname,vadd,vcontact,vcreditdays,vdate\n",
        "2001,\"KIRAN PATIL\",PUNE,20535535,20,20-JAN-10\n",
        "2002,\"PRAKASH JAIN\",MUNBAI,2063564,10,05-NOV-11\n",
        "2003,\"SWAPNIL THETE\",NASHIK,25355352,15,18-MAR-10\n",
        "2004,\"AMOL SHENDE\",SATARA,23674776,18,07-APR-15\n",
        "2005,\"KARAN SINTRE\",BULDHANA,256756,30,22-APR-09\n",
        "2006,\"RAM KULKARNI\",OSMANABAD,20367454,21,09-JUL-10\n",
        "\n",
        "# Spark Session Setup and Data Loading\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType\n",
        "from num2words import num2words\n",
        "\n",
        "spark = SparkSession.builder.appName(\"GarageAnalysis\").getOrCreate()\n",
        "\n",
        "customer_df = spark.read.csv(\"customer_table.csv\", header=True, inferSchema=True)\n",
        "ser_det_df = spark.read.csv(\"ser_det_table.csv\", header=True, inferSchema=True)\n",
        "employee_df = spark.read.csv(\"employee_table.csv\", header=True, inferSchema=True)\n",
        "sparepart_df = spark.read.csv(\"sparepart_table.csv\", header=True, inferSchema=True)\n",
        "purchase_df = spark.read.csv(\"purchase_table.csv\", header=True, inferSchema=True)\n",
        "vendor_df = spark.read.csv(\"vendor_table.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Create Temporary Views for Spark SQL\n",
        "customer_df.createOrReplaceTempView(\"customer_table\")\n",
        "ser_det_df.createOrReplaceTempView(\"ser_det_table\")\n",
        "employee_df.createOrReplaceTempView(\"employee_table\")\n",
        "sparepart_df.createOrReplaceTempView(\"sparepart_table\")\n",
        "purchase_df.createOrReplaceTempView(\"purchase_table\")\n",
        "vendor_df.createOrReplaceTempView(\"vendor_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c575a60b",
      "metadata": {
        "id": "c575a60b"
      },
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# Garage Data System: Dual PySpark & SQL Analysis\n",
        "# =====================================================================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType\n",
        "from num2words import num2words  # You may need to run: pip install num2words\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. Spark Session and Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "spark = SparkSession.builder.appName(\"GarageAnalysis\").getOrCreate()\n",
        "\n",
        "# Load data into DataFrames (expects CSV files in the working directory)\n",
        "customer_df = spark.read.csv(\"customer_table.csv\", header=True, inferSchema=True)\n",
        "ser_det_df = spark.read.csv(\"ser_det_table.csv\", header=True, inferSchema=True)\n",
        "employee_df = spark.read.csv(\"employee_table.csv\", header=True, inferSchema=True)\n",
        "sparepart_df = spark.read.csv(\"sparepart_table.csv\", header=True, inferSchema=True)\n",
        "purchase_df = spark.read.csv(\"purchase_table.csv\", header=True, inferSchema=True)\n",
        "vendor_df = spark.read.csv(\"vendor_table.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Create Temporary Views for Spark SQL\n",
        "# ---------------------------------------------------------------------\n",
        "customer_df.createOrReplaceTempView(\"customer_table\")\n",
        "ser_det_df.createOrReplaceTempView(\"ser_det_table\")\n",
        "employee_df.createOrReplaceTempView(\"employee_table\")\n",
        "sparepart_df.createOrReplaceTempView(\"sparepart_table\")\n",
        "purchase_df.createOrReplaceTempView(\"purchase_table\")\n",
        "vendor_df.createOrReplaceTempView(\"vendor_table\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41b8f9b",
      "metadata": {
        "id": "a41b8f9b"
      },
      "source": [
        "## <a id='q1'></a>Q.1 List all the customers serviced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45b1a67",
      "metadata": {
        "id": "c45b1a67"
      },
      "outputs": [],
      "source": [
        "# Q.1 List all the customers serviced.\n",
        "# -- DataFrame API Solution --\n",
        "customer_df.join(ser_det_df, customer_df.cid == ser_det_df.cid, \"inner\").select(\"cname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe71e98e",
      "metadata": {
        "id": "fe71e98e"
      },
      "source": [
        "## <a id='q2'></a>Q.2 Customers who are not serviced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604fb973",
      "metadata": {
        "id": "604fb973"
      },
      "outputs": [],
      "source": [
        "# Q.2 Customers who are not serviced.\n",
        "# -- DataFrame API Solution --\n",
        "customer_df.join(ser_det_df, customer_df.cid == ser_det_df.cid, \"left_anti\").select(\"cname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname\n",
        "    FROM customer_table c\n",
        "    LEFT ANTI JOIN ser_det_table s ON c.cid = s.cid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f166924f",
      "metadata": {
        "id": "f166924f"
      },
      "source": [
        "## <a id='q3'></a>Q.3 Employees who have not received the commission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc76d449",
      "metadata": {
        "id": "cc76d449"
      },
      "outputs": [],
      "source": [
        "# Q.3 Employees who have not received the commission.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.join(ser_det_df, employee_df.eid == ser_det_df.eid).where(F.col(\"comm\") == 0).select(\"ename\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT e.ename\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid=s.eid\n",
        "    WHERE s.comm = 0\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f1e319b",
      "metadata": {
        "id": "2f1e319b"
      },
      "source": [
        "## <a id='q4'></a>Q.4 Name the employee who have maximum Commission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de37c5d",
      "metadata": {
        "id": "9de37c5d"
      },
      "outputs": [],
      "source": [
        "# Q.4 Name the employee who have maximum Commission.\n",
        "# -- DataFrame API Solution --\n",
        "max_comm = ser_det_df.agg(F.max(\"comm\")).collect()[0][0]\n",
        "employee_df.join(ser_det_df, employee_df.eid == ser_det_df.eid).where(F.col(\"comm\") == max_comm).select(\"ename\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid=s.eid\n",
        "    WHERE s.comm = (SELECT MAX(comm) FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c24880",
      "metadata": {
        "id": "25c24880"
      },
      "source": [
        "## <a id='q5'></a>Q.5 Show employee name and minimum commission amount received by an employee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113b789a",
      "metadata": {
        "id": "113b789a"
      },
      "outputs": [],
      "source": [
        "# Q.5 Show employee name and minimum commission amount received by an employee.\n",
        "# -- DataFrame API Solution --\n",
        "min_comm = ser_det_df.where(F.col(\"comm\") > 0).agg(F.min(\"comm\")).collect()[0][0]\n",
        "employee_df.join(ser_det_df, employee_df.eid == ser_det_df.eid).where(F.col(\"comm\") == min_comm).select(\"ename\", \"comm\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, s.comm\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    WHERE s.comm = (SELECT MIN(comm) FROM ser_det_table WHERE comm > 0)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714cc0f6",
      "metadata": {
        "id": "714cc0f6"
      },
      "source": [
        "## <a id='q6'></a>Q.6 Display the Middle record from any table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002b5805",
      "metadata": {
        "id": "002b5805"
      },
      "outputs": [],
      "source": [
        "# Q.6 Display the Middle record from any table.\n",
        "# -- DataFrame API Solution --\n",
        "total_count = ser_det_df.count()\n",
        "middle_row_index = (total_count // 2) + 1\n",
        "window_spec = Window.orderBy(\"sid\")\n",
        "ser_det_df.withColumn(\"rn\", F.row_number().over(window_spec)).where(F.col(\"rn\") == middle_row_index).drop(\"rn\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT * FROM (\n",
        "        SELECT *, ROW_NUMBER() OVER (ORDER BY sid) as rn, COUNT(*) OVER() as total_count\n",
        "        FROM ser_det_table\n",
        "    )\n",
        "    WHERE rn = CAST(CEIL(total_count / 2.0) AS INT)\n",
        "\"\"\").show(1, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff8abda",
      "metadata": {
        "id": "2ff8abda"
      },
      "source": [
        "## <a id='q7'></a>Q.7 Display last 4 records of any table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61fc4ce6",
      "metadata": {
        "id": "61fc4ce6"
      },
      "outputs": [],
      "source": [
        "# Q.7 Display last 4 records of any table.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.orderBy(F.col(\"eid\").desc()).limit(4).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT * FROM employee_table ORDER BY eid DESC LIMIT 4\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b937ab",
      "metadata": {
        "id": "26b937ab"
      },
      "source": [
        "## <a id='q8'></a>Q.8 Count the number of records without count function from any table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4d98ae",
      "metadata": {
        "id": "3c4d98ae"
      },
      "outputs": [],
      "source": [
        "# Q.8 Count the number of records without count function from any table.\n",
        "# -- DataFrame API Solution --\n",
        "window_spec = Window.orderBy(\"eid\")\n",
        "count_val = employee_df.withColumn(\"rn\", F.row_number().over(window_spec)).agg(F.max(\"rn\")).collect()[0][0]\n",
        "print(f\"Record count for employee_table: {count_val}\")\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT MAX(rn) as count FROM (SELECT ROW_NUMBER() OVER (ORDER BY eid) as rn FROM employee_table)\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1ae0ff",
      "metadata": {
        "id": "1f1ae0ff"
      },
      "source": [
        "## <a id='q9'></a>Q.9 Show unique records from \"Ser_det\" table on cid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa78227a",
      "metadata": {
        "id": "aa78227a"
      },
      "outputs": [],
      "source": [
        "# Q.9 Show unique records from \"Ser_det\" table on cid.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.dropDuplicates([\"cid\"]).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT * FROM (\n",
        "        SELECT *, ROW_NUMBER() OVER(PARTITION BY cid ORDER BY sid) as rn\n",
        "        FROM ser_det_table\n",
        "    ) WHERE rn = 1\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd5d453",
      "metadata": {
        "id": "0cd5d453"
      },
      "source": [
        "## <a id='q10'></a>Q.10 Show the name of Customer who have paid maximum amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67a8265",
      "metadata": {
        "id": "f67a8265"
      },
      "outputs": [],
      "source": [
        "# Q.10 Show the name of Customer who have paid maximum amount.\n",
        "# -- DataFrame API Solution --\n",
        "max_total = ser_det_df.agg(F.max(\"total\")).collect()[0][0]\n",
        "customer_df.join(ser_det_df, customer_df.cid == ser_det_df.cid).where(F.col(\"total\") == max_total).select(\"cname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE s.total = (SELECT MAX(total) FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bde176",
      "metadata": {
        "id": "40bde176"
      },
      "source": [
        "## <a id='q11'></a>Q.11 Display Employees who are not currently working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efca5b31",
      "metadata": {
        "id": "efca5b31"
      },
      "outputs": [],
      "source": [
        "# Q.11 Display Employees who are not currently working.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.where(F.col(\"edol\").isNotNull()).select(\"ename\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT ename FROM employee_table WHERE edol IS NOT NULL\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5a48e4",
      "metadata": {
        "id": "7a5a48e4"
      },
      "source": [
        "## <a id='q12'></a>Q.12 How many customers serviced their two wheelers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af68f594",
      "metadata": {
        "id": "af68f594"
      },
      "outputs": [],
      "source": [
        "# Q.12 How many customers serviced their two wheelers.\n",
        "# -- DataFrame API Solution --\n",
        "count = ser_det_df.where(F.lower(F.col(\"type_veh\")) == \"two wheeler\").select(\"cid\").distinct().count()\n",
        "print(f\"Number of customers who serviced two-wheelers: {count}\")\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT COUNT(DISTINCT cid) AS two_wheeler_customers\n",
        "    FROM ser_det_table\n",
        "    WHERE type_veh = 'TWO WHEELER'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6325c5e5",
      "metadata": {
        "id": "6325c5e5"
      },
      "source": [
        "## <a id='q13'></a>Q.13 List the Purchased Items which are used for Customer Service with Unit of that Item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56aa0e53",
      "metadata": {
        "id": "56aa0e53"
      },
      "outputs": [],
      "source": [
        "# Q.13 List the Purchased Items which are used for Customer Service with Unit of that Item.\n",
        "# -- DataFrame API Solution --\n",
        "sparepart_df.join(ser_det_df, sparepart_df.spid == ser_det_df.spid, \"inner\").select(\"spname\", \"spunit\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT sp.spname, sp.spunit\n",
        "    FROM sparepart_table sp\n",
        "    JOIN ser_det_table s ON sp.spid = s.spid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0920fecd",
      "metadata": {
        "id": "0920fecd"
      },
      "source": [
        "## <a id='q14'></a>Q.14 Customers who have Colored their vehicles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f41f29",
      "metadata": {
        "id": "59f41f29"
      },
      "outputs": [],
      "source": [
        "# Q.14 Customers who have Colored their vehicles.\n",
        "# -- DataFrame API Solution --\n",
        "customer_df.join(ser_det_df, customer_df.cid == ser_det_df.cid).where(F.lower(F.col(\"typ_ser\")) == \"color\").select(\"cname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE s.typ_ser = 'COLOR'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c3eb3f",
      "metadata": {
        "id": "b5c3eb3f"
      },
      "source": [
        "## <a id='q15'></a>Q.15 Find the annual income of each employee inclusive of Commission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6864b5f",
      "metadata": {
        "id": "d6864b5f"
      },
      "outputs": [],
      "source": [
        "# Q.15 Find the annual income of each employee inclusive of Commission.\n",
        "# -- DataFrame API Solution --\n",
        "commission_agg = ser_det_df.groupBy(\"eid\").agg(F.sum(\"comm\").alias(\"total_comm\"))\n",
        "employee_df.join(commission_agg, \"eid\", \"left\").na.fill(0, [\"total_comm\"]).withColumn(\"annual_income\", (F.col(\"esal\") * 12) + F.col(\"total_comm\")).select(\"ename\", \"annual_income\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, (e.esal * 12) + COALESCE(s.total_comm, 0) AS annual_income\n",
        "    FROM employee_table e\n",
        "    LEFT JOIN (SELECT eid, SUM(comm) as total_comm FROM ser_det_table GROUP BY eid) s ON e.eid = s.eid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f031dda3",
      "metadata": {
        "id": "f031dda3"
      },
      "source": [
        "## <a id='q16'></a>Q.16 Vendor Names who provides the engine oil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185e1229",
      "metadata": {
        "id": "185e1229"
      },
      "outputs": [],
      "source": [
        "# Q.16 Vendor Names who provides the engine oil.\n",
        "# -- DataFrame API Solution --\n",
        "vendor_df.join(purchase_df, \"vid\").join(sparepart_df, \"spid\").where(F.col(\"spname\").like(\"%ENGINE OIL%\")).select(\"vname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT v.vname\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    JOIN sparepart_table sp ON p.spid = sp.spid\n",
        "    WHERE sp.spname LIKE '%ENGINE OIL%'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef788bb",
      "metadata": {
        "id": "3ef788bb"
      },
      "source": [
        "## <a id='q17'></a>Q.17 Total Cost to purchase the Color and name the color purchased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e9c40f",
      "metadata": {
        "id": "78e9c40f"
      },
      "outputs": [],
      "source": [
        "# Q.17 Total Cost to purchase the Color and name the color purchased.\n",
        "# -- DataFrame API Solution --\n",
        "purchase_df.join(sparepart_df, \"spid\").where(F.col(\"spname\").like(\"%COLOUR%\")).groupBy(\"spname\").agg(F.sum(\"total\").alias(\"total_purchase_cost\")).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT sp.spname, SUM(p.total) AS total_purchase_cost\n",
        "    FROM purchase_table p\n",
        "    JOIN sparepart_table sp ON p.spid = sp.spid\n",
        "    WHERE sp.spname LIKE '%COLOUR%'\n",
        "    GROUP BY sp.spname\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ae47be",
      "metadata": {
        "id": "c3ae47be"
      },
      "source": [
        "## <a id='q18'></a>Q.18 Purchased Items which are not used in \"Ser_det\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39139c0d",
      "metadata": {
        "id": "39139c0d"
      },
      "outputs": [],
      "source": [
        "# Q.18 Purchased Items which are not used in \"Ser_det\".\n",
        "# -- DataFrame API Solution --\n",
        "purchased_spids = purchase_df.select(\"spid\").distinct()\n",
        "serviced_spids = ser_det_df.select(\"spid\").distinct()\n",
        "unused_spids = purchased_spids.subtract(serviced_spids)\n",
        "unused_spids.join(sparepart_df, \"spid\").select(\"spname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT sp.spname\n",
        "    FROM sparepart_table sp\n",
        "    WHERE sp.spid IN (SELECT spid FROM purchase_table)\n",
        "      AND sp.spid NOT IN (SELECT spid FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5689cafd",
      "metadata": {
        "id": "5689cafd"
      },
      "source": [
        "## <a id='q19'></a>Q.19 Spare Parts Not Purchased but existing in Sparepart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbfcbdb",
      "metadata": {
        "id": "acbfcbdb"
      },
      "outputs": [],
      "source": [
        "# Q.19 Spare Parts Not Purchased but existing in Sparepart.\n",
        "# -- DataFrame API Solution --\n",
        "sparepart_df.join(purchase_df, \"spid\", \"left_anti\").select(\"spname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT spname FROM sparepart_table\n",
        "    WHERE spid NOT IN (SELECT spid FROM purchase_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311f4428",
      "metadata": {
        "id": "311f4428"
      },
      "source": [
        "## <a id='q20'></a>Q.20 Calculate the Profit/Loss of the Firm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0968d0",
      "metadata": {
        "id": "8c0968d0"
      },
      "outputs": [],
      "source": [
        "# Q.20 Calculate the Profit/Loss of the Firm.\n",
        "# -- DataFrame API Solution --\n",
        "total_revenue = ser_det_df.agg(F.sum(\"total\")).collect()[0][0]\n",
        "total_salary_cost = employee_df.agg(F.sum(\"esal\")).collect()[0][0]\n",
        "total_purchase_cost = purchase_df.agg(F.sum(\"total\")).collect()[0][0]\n",
        "profit_loss = total_revenue - (total_salary_cost + total_purchase_cost)\n",
        "print(f\"Firm Profit/Loss: {profit_loss}\")\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        (SELECT SUM(total) FROM ser_det_table) -\n",
        "        ((SELECT SUM(esal) FROM employee_table) + (SELECT SUM(total) FROM purchase_table))\n",
        "        AS profit_or_loss\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b692ff0e",
      "metadata": {
        "id": "b692ff0e"
      },
      "source": [
        "## <a id='q21'></a>Q.21 Specify the names of customers who have serviced their vehicles more than one time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b337fe5",
      "metadata": {
        "id": "9b337fe5"
      },
      "outputs": [],
      "source": [
        "# Q.21 Specify the names of customers who have serviced their vehicles more than one time.\n",
        "# -- DataFrame API Solution --\n",
        "customer_counts = ser_det_df.groupBy(\"cid\").count().where(F.col(\"count\") > 1)\n",
        "customer_counts.join(customer_df, \"cid\").select(\"cname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    GROUP BY c.cid, c.cname\n",
        "    HAVING COUNT(s.sid) > 1\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba97552",
      "metadata": {
        "id": "8ba97552"
      },
      "source": [
        "## <a id='q22'></a>Q.22 List the Items purchased from vendors locationwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c02ce2",
      "metadata": {
        "id": "20c02ce2"
      },
      "outputs": [],
      "source": [
        "# Q.22 List the Items purchased from vendors locationwise.\n",
        "# -- DataFrame API Solution --\n",
        "vendor_df.join(purchase_df, \"vid\").join(sparepart_df, \"spid\").select(F.col(\"vadd\").alias(\"location\"), F.col(\"spname\").alias(\"item_name\")).orderBy(\"location\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vadd AS location, sp.spname AS item_name\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    JOIN sparepart_table sp ON p.spid = sp.spid\n",
        "    ORDER BY v.vadd\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608ef0e2",
      "metadata": {
        "id": "608ef0e2"
      },
      "source": [
        "## <a id='q23'></a>Q.23 Display count of two wheeler and four wheeler from ser_details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929971b1",
      "metadata": {
        "id": "929971b1"
      },
      "outputs": [],
      "source": [
        "# Q.23 Display count of two wheeler and four wheeler from ser_details.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.groupBy(\"type_veh\").count().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT type_veh, COUNT(*)\n",
        "    FROM ser_det_table\n",
        "    GROUP BY type_veh\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5387ee4a",
      "metadata": {
        "id": "5387ee4a"
      },
      "source": [
        "## <a id='q24'></a>Q.24 Display name of customers who paid highest SPGST and for which item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10172df5",
      "metadata": {
        "id": "10172df5"
      },
      "outputs": [],
      "source": [
        "# Q.24 Display name of customers who paid highest SPGST and for which item.\n",
        "# -- DataFrame API Solution --\n",
        "max_spgst = purchase_df.agg(F.max(\"spgst\")).collect()[0][0]\n",
        "spid_with_max_spgst = purchase_df.where(F.col(\"spgst\") == max_spgst).select(\"spid\")\n",
        "ser_det_df.join(spid_with_max_spgst, \"spid\").join(customer_df, \"cid\").join(sparepart_df, \"spid\").select(\"cname\", \"spname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c.cname, sp.spname\n",
        "    FROM ser_det_table s\n",
        "    JOIN customer_table c ON s.cid = c.cid\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "    WHERE s.spid IN (\n",
        "        SELECT spid FROM purchase_table WHERE spgst = (SELECT MAX(spgst) FROM purchase_table)\n",
        "    )\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c18837a",
      "metadata": {
        "id": "2c18837a"
      },
      "source": [
        "## <a id='q25'></a>Q.25 Display vendors name who have charged highest SPGST rate for which item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ade6e2e",
      "metadata": {
        "id": "2ade6e2e"
      },
      "outputs": [],
      "source": [
        "# Q.25 Display vendors name who have charged highest SPGST rate for which item.\n",
        "# -- DataFrame API Solution --\n",
        "max_spgst = purchase_df.agg(F.max(\"spgst\")).collect()[0][0]\n",
        "purchase_df.where(F.col(\"spgst\") == max_spgst).join(vendor_df, \"vid\").join(sparepart_df, \"spid\").select(\"vname\", \"spname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vname, sp.spname\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    JOIN sparepart_table sp ON p.spid = sp.spid\n",
        "    WHERE p.spgst = (SELECT MAX(spgst) FROM purchase_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eab5268",
      "metadata": {
        "id": "0eab5268"
      },
      "source": [
        "## <a id='q26'></a>Q.26 List name of item and employee name who have received item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544f7d5d",
      "metadata": {
        "id": "544f7d5d"
      },
      "outputs": [],
      "source": [
        "# Q.26 List name of item and employee name who have received item.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.join(ser_det_df, \"eid\").join(sparepart_df, \"spid\").select(\"ename\", \"spname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT e.ename, sp.spname\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb80780",
      "metadata": {
        "id": "0bb80780"
      },
      "source": [
        "## <a id='q27'></a>Q.27 Display Customer Name, Vehicle Number, Item Used, Purchase Date, Vendor, and Location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e887d6d",
      "metadata": {
        "id": "2e887d6d"
      },
      "outputs": [],
      "source": [
        "# Q.27 Display Customer Name, Vehicle Number, Item Used, Purchase Date, Vendor, and Location.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.join(customer_df, \"cid\").join(sparepart_df, \"spid\").join(purchase_df, \"spid\").join(vendor_df, \"vid\").select(\"cname\", \"veh_no\", \"spname\", \"pdate\", \"vname\", \"vadd\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c.cname, s.veh_no, sp.spname, p.pdate, v.vname, v.vadd\n",
        "    FROM ser_det_table s\n",
        "    JOIN customer_table c ON s.cid = c.cid\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "    JOIN purchase_table p ON s.spid = p.spid\n",
        "    JOIN vendor_table v ON p.vid = v.vid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3653f79b",
      "metadata": {
        "id": "3653f79b"
      },
      "source": [
        "## <a id='q28'></a>Q.28 who belong this vehicle 'MH14PA335'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd0d705",
      "metadata": {
        "id": "8dd0d705"
      },
      "outputs": [],
      "source": [
        "# Q.28 who belong this vehicle 'MH14PA335'?\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.where(F.col(\"veh_no\") == 'MH14PA335').join(customer_df, \"cid\").select(\"cname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE s.veh_no = 'MH14PA335'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7256840",
      "metadata": {
        "id": "c7256840"
      },
      "source": [
        "## <a id='q29'></a>Q.29 Display the name of customer from New York and their vehicle service date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76544a85",
      "metadata": {
        "id": "76544a85"
      },
      "outputs": [],
      "source": [
        "# Q.29 Display the name of customer from New York and their vehicle service date.\n",
        "# -- DataFrame API Solution --\n",
        "customer_df.where(F.col(\"cadd\") == 'NEW YORK').join(ser_det_df, \"cid\").select(\"cname\", \"ser_date\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname, s.ser_date\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE c.cadd = 'NEW YORK'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7109d956",
      "metadata": {
        "id": "7109d956"
      },
      "source": [
        "## <a id='q30'></a>Q.30 from whom we have purchased items having maximum cost?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfac6cd6",
      "metadata": {
        "id": "cfac6cd6"
      },
      "outputs": [],
      "source": [
        "# Q.30 from whom we have purchased items having maximum cost?\n",
        "# -- DataFrame API Solution --\n",
        "max_purchase_total = purchase_df.agg(F.max(\"total\")).collect()[0][0]\n",
        "purchase_df.where(F.col(\"total\") == max_purchase_total).join(vendor_df, \"vid\").select(\"vname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vname\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    WHERE p.total = (SELECT MAX(total) FROM purchase_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd787d2",
      "metadata": {
        "id": "ddd787d2"
      },
      "source": [
        "## <a id='q31'></a>Q.31 Display employees who are not 'Mechanic' and have done services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c776a9",
      "metadata": {
        "id": "61c776a9"
      },
      "outputs": [],
      "source": [
        "# Q.31 Display employees who are not 'Mechanic' and have done services.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.where(F.col(\"ejob\") != 'MECHANIC').join(ser_det_df, \"eid\", \"inner\").select(\"ename\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT e.ename\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    WHERE e.ejob != 'MECHANIC'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1560a54",
      "metadata": {
        "id": "d1560a54"
      },
      "source": [
        "## <a id='q32'></a>Q.32 Display jobs with more than two employees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15f6fd9",
      "metadata": {
        "id": "e15f6fd9"
      },
      "outputs": [],
      "source": [
        "# Q.32 Display jobs with more than two employees.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.groupBy(\"ejob\").count().where(F.col(\"count\") > 1).show()  # > 1 for sample data\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ejob, COUNT(*)\n",
        "    FROM employee_table\n",
        "    GROUP BY ejob\n",
        "    HAVING COUNT(*) > 1\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91aec302",
      "metadata": {
        "id": "91aec302"
      },
      "source": [
        "## <a id='q33'></a>Q.33 Display details of employees who did a service and rank them by number of services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849bbf64",
      "metadata": {
        "id": "849bbf64"
      },
      "outputs": [],
      "source": [
        "# Q.33 Display details of employees who did a service and rank them by number of services.\n",
        "# -- DataFrame API Solution --\n",
        "service_counts = ser_det_df.groupBy(\"eid\").count()\n",
        "window_spec = Window.orderBy(F.col(\"count\").desc())\n",
        "employee_df.join(service_counts, \"eid\").select(\"ename\", \"ejob\", \"count\").withColumn(\"service_rank\", F.rank().over(window_spec)).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, e.ejob, COUNT(s.sid) AS service_count,\n",
        "           RANK() OVER (ORDER BY COUNT(s.sid) DESC) AS service_rank\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    GROUP BY e.eid, e.ename, e.ejob\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311596d5",
      "metadata": {
        "id": "311596d5"
      },
      "source": [
        "## <a id='q34'></a>Q.34 Display painter/fitter employees who provided a service and total service count for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9cd844",
      "metadata": {
        "id": "de9cd844"
      },
      "outputs": [],
      "source": [
        "# Q.34 Display painter/fitter employees who provided a service and total service count for each.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.where(F.col(\"ejob\").isin(['PAINTER', 'FITTER'])).join(ser_det_df, \"eid\", \"inner\").groupBy(\"ename\", \"ejob\").count().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, e.ejob, COUNT(s.sid) AS service_count\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    WHERE e.ejob IN ('PAINTER', 'FITTER')\n",
        "    GROUP BY e.eid, e.ename, e.ejob\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d895b898",
      "metadata": {
        "id": "d895b898"
      },
      "source": [
        "## <a id='q35'></a>Q.35 Display employee salary and provide a Grade based on salary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ec7693",
      "metadata": {
        "id": "60ec7693"
      },
      "outputs": [],
      "source": [
        "# Q.35 Display employee salary and provide a Grade based on salary.\n",
        "# -- DataFrame API Solution --\n",
        "window_spec = Window.orderBy(F.col(\"esal\").desc().nulls_last())\n",
        "employee_df.withColumn(\"rank\", F.dense_rank().over(window_spec)).withColumn(\"salary_grade\", F.when(F.col(\"rank\") <= 2, 'A').when(F.col(\"rank\") <= 4, 'B').otherwise('C')).select(\"ename\", \"esal\", \"salary_grade\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ename, esal,\n",
        "        CASE\n",
        "            WHEN DENSE_RANK() OVER (ORDER BY esal DESC NULLS LAST) <= 2 THEN 'A'\n",
        "            WHEN DENSE_RANK() OVER (ORDER BY esal DESC NULLS LAST) <= 4 THEN 'B'\n",
        "            ELSE 'C'\n",
        "        END AS salary_grade\n",
        "    FROM employee_table\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61dfd0d1",
      "metadata": {
        "id": "61dfd0d1"
      },
      "source": [
        "## <a id='q36'></a>Q.36 display the 4th record of emp table without using group by and rowid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b09348",
      "metadata": {
        "id": "a1b09348"
      },
      "outputs": [],
      "source": [
        "# Q.36 display the 4th record of emp table without using group by and rowid.\n",
        "# -- DataFrame API Solution --\n",
        "window_spec = Window.orderBy(\"eid\")\n",
        "employee_df.withColumn(\"rn\", F.row_number().over(window_spec)).where(F.col(\"rn\") == 4).drop(\"rn\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT * FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY eid) as rn FROM employee_table) WHERE rn = 4\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68de1c8",
      "metadata": {
        "id": "f68de1c8"
      },
      "source": [
        "## <a id='q37'></a>Q.37 Provide a commission 100 to employees who are not earning any commission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136b146a",
      "metadata": {
        "id": "136b146a"
      },
      "outputs": [],
      "source": [
        "# Q.37 Provide a commission 100 to employees who are not earning any commission.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.withColumn(\"new_commission\", F.when(F.col(\"comm\") == 0, 100).otherwise(F.col(\"comm\"))).select(\"sid\", \"eid\", \"comm\", \"new_commission\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT sid, eid, comm,\n",
        "           CASE WHEN comm = 0 THEN 100 ELSE comm END AS new_commission\n",
        "    FROM ser_det_table\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a42c932",
      "metadata": {
        "id": "0a42c932"
      },
      "source": [
        "## <a id='q38'></a>Q.38 totals no. of services for each day and place the results in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3334ff",
      "metadata": {
        "id": "bf3334ff"
      },
      "outputs": [],
      "source": [
        "# Q.38 totals no. of services for each day and place the results in descending order.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.groupBy(\"ser_date\").count().orderBy(F.col(\"count\").desc()).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ser_date, COUNT(sid) AS number_of_services\n",
        "    FROM ser_det_table\n",
        "    GROUP BY ser_date\n",
        "    ORDER BY number_of_services DESC\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa58eee8",
      "metadata": {
        "id": "fa58eee8"
      },
      "source": [
        "## <a id='q39'></a>Q.39 Display the service details of those customer who belong from same city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbbfcb3",
      "metadata": {
        "id": "0bbbfcb3"
      },
      "outputs": [],
      "source": [
        "# Q.39 Display the service details of those customer who belong from same city.\n",
        "# -- DataFrame API Solution --\n",
        "city_counts = customer_df.groupBy(\"cadd\").count().where(F.col(\"count\") > 1)\n",
        "multi_cust_cities = city_counts.select(\"cadd\")\n",
        "customer_df.join(multi_cust_cities, \"cadd\").join(ser_det_df, \"cid\").select(ser_det_df[\"*\"]).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT s.*\n",
        "    FROM ser_det_table s\n",
        "    JOIN customer_table c1 ON s.cid = c1.cid\n",
        "    WHERE c1.cadd IN (\n",
        "        SELECT cadd FROM customer_table GROUP BY cadd HAVING COUNT(*) > 1\n",
        "    )\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6439debb",
      "metadata": {
        "id": "6439debb"
      },
      "source": [
        "## <a id='q40'></a>Q.40 find all pairs of customers serviced by a single employee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d356c5a",
      "metadata": {
        "id": "7d356c5a"
      },
      "outputs": [],
      "source": [
        "# Q.40 find all pairs of customers serviced by a single employee.\n",
        "# -- DataFrame API Solution --\n",
        "s1 = ser_det_df.alias(\"s1\")\n",
        "s2 = ser_det_df.alias(\"s2\")\n",
        "c1 = customer_df.alias(\"c1\")\n",
        "c2 = customer_df.alias(\"c2\")\n",
        "s1.join(s2, (s1.eid == s2.eid) & (s1.cid < s2.cid)).join(c1, s1.cid == c1.cid).join(c2, s2.cid == c2.cid).select(c1.cname.alias(\"customer1\"), c2.cname.alias(\"customer2\"), s1.eid).distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c1.cname AS customer1, c2.cname AS customer2, s1.eid AS employee_id\n",
        "    FROM ser_det_table s1\n",
        "    JOIN ser_det_table s2 ON s1.eid = s2.eid AND s1.cid < s2.cid\n",
        "    JOIN customer_table c1 ON s1.cid = c1.cid\n",
        "    JOIN customer_table c2 ON s2.cid = c2.cid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5a92fe",
      "metadata": {
        "id": "3f5a92fe"
      },
      "source": [
        "## <a id='q41'></a>Q.41 List each service number follow by name of the customer who made that service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59e432a",
      "metadata": {
        "id": "b59e432a"
      },
      "outputs": [],
      "source": [
        "# Q.41 List each service number follow by name of the customer who made that service.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.join(customer_df, \"cid\").select(\"sid\", \"cname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT s.sid, c.cname\n",
        "    FROM ser_det_table s\n",
        "    JOIN customer_table c ON s.cid = c.cid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d1d642",
      "metadata": {
        "id": "43d1d642"
      },
      "source": [
        "## <a id='q42'></a>Q.42 Provide employee rating (A,B,C,D) based on number of services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62d8af2",
      "metadata": {
        "id": "e62d8af2"
      },
      "outputs": [],
      "source": [
        "# Q.42 Provide employee rating (A,B,C,D) based on number of services.\n",
        "# -- DataFrame API Solution --\n",
        "service_counts_df = ser_det_df.groupBy(\"eid\").count()\n",
        "employee_df.join(service_counts_df, \"eid\", \"left\").na.fill(0).withColumn(\"rating\", F.when(F.col(\"count\") >= 3, 'A').when(F.col(\"count\") == 2, 'B').when(F.col(\"count\") == 1, 'C').otherwise('D')).select(\"ename\", \"ejob\", \"count\", \"rating\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, e.ejob, sc.service_count,\n",
        "        CASE\n",
        "            WHEN sc.service_count >= 3 THEN 'A'\n",
        "            WHEN sc.service_count = 2 THEN 'B'\n",
        "            WHEN sc.service_count = 1 THEN 'C'\n",
        "            ELSE 'D'\n",
        "        END as rating\n",
        "    FROM employee_table e\n",
        "    LEFT JOIN (SELECT eid, COUNT(*) as service_count FROM ser_det_table GROUP BY eid) sc ON e.eid = sc.eid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cfcfe8",
      "metadata": {
        "id": "b1cfcfe8"
      },
      "source": [
        "## <a id='q43'></a>Q.43 Get maximum service amount of each customer with their customer details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca84032b",
      "metadata": {
        "id": "ca84032b"
      },
      "outputs": [],
      "source": [
        "# Q.43 Get maximum service amount of each customer with their customer details.\n",
        "# -- DataFrame API Solution --\n",
        "max_service_amt_df = ser_det_df.groupBy(\"cid\").agg(F.max(\"total\").alias(\"max_service_amount\"))\n",
        "customer_df.join(max_service_amt_df, \"cid\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.*, MAX(s.total) as max_service_amount\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    GROUP BY c.cid, c.cname, c.cadd, c.contact, c.creditdays, c.date, c.gender\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5379876a",
      "metadata": {
        "id": "5379876a"
      },
      "source": [
        "## <a id='q44'></a>Q.44 Get the details of customers with his total no of services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454c156f",
      "metadata": {
        "id": "454c156f"
      },
      "outputs": [],
      "source": [
        "# Q.44 Get the details of customers with his total no of services.\n",
        "# -- DataFrame API Solution --\n",
        "total_services_df = ser_det_df.groupBy(\"cid\").count().withColumnRenamed(\"count\", \"total_services\")\n",
        "customer_df.join(total_services_df, \"cid\", \"left\").na.fill(0).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.*, COUNT(s.sid) AS total_services\n",
        "    FROM customer_table c\n",
        "    LEFT JOIN ser_det_table s ON c.cid = s.cid\n",
        "    GROUP BY c.cid, c.cname, c.cadd, c.contact, c.creditdays, c.date, c.gender\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148cf8ba",
      "metadata": {
        "id": "148cf8ba"
      },
      "source": [
        "## <a id='q45'></a>Q.45 From which location sparepart purchased with highest cost?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd60452a",
      "metadata": {
        "id": "bd60452a"
      },
      "outputs": [],
      "source": [
        "# Q.45 From which location sparepart purchased with highest cost?\n",
        "# -- DataFrame API Solution --\n",
        "max_sprate = purchase_df.agg(F.max(\"sprate\")).collect()[0][0]\n",
        "purchase_df.where(F.col(\"sprate\") == max_sprate).join(vendor_df, \"vid\").select(\"vadd\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vadd\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    WHERE p.sprate = (SELECT MAX(sprate) FROM purchase_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99499ff1",
      "metadata": {
        "id": "99499ff1"
      },
      "source": [
        "## <a id='q46'></a>Q.46 Get the details of employee with their service details who has salary is null."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c18409",
      "metadata": {
        "id": "10c18409"
      },
      "outputs": [],
      "source": [
        "# Q.46 Get the details of employee with their service details who has salary is null.\n",
        "# -- DataFrame API Solution --\n",
        "employee_df.where(F.col(\"esal\").isNull()).join(ser_det_df, \"eid\").select(employee_df.ename, employee_df.ejob, ser_det_df[\"*\"]).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename, e.ejob, s.*\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    WHERE e.esal IS NULL\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae7427b",
      "metadata": {
        "id": "2ae7427b"
      },
      "source": [
        "## <a id='q47'></a>Q.47 find the sum of purchase location wise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e1bf1a",
      "metadata": {
        "id": "40e1bf1a"
      },
      "outputs": [],
      "source": [
        "# Q.47 find the sum of purchase location wise.\n",
        "# -- DataFrame API Solution --\n",
        "vendor_df.join(purchase_df, \"vid\").groupBy(\"vadd\").agg(F.sum(\"total\").alias(\"total_purchase_amount\")).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vadd, SUM(p.total) AS total_purchase_amount\n",
        "    FROM vendor_table v\n",
        "    JOIN purchase_table p ON v.vid = p.vid\n",
        "    GROUP BY v.vadd\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88116190",
      "metadata": {
        "id": "88116190"
      },
      "source": [
        "## <a id='q48'></a>Q.48 write a query sum of purchase amount in word location wise?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a909a4",
      "metadata": {
        "id": "18a909a4"
      },
      "outputs": [],
      "source": [
        "# Q.48 write a query sum of purchase amount in word location wise?\n",
        "# -- DataFrame API Solution --\n",
        "def number_to_words_py(n):\n",
        "    if n is None:\n",
        "        return None\n",
        "    return num2words(int(n))\n",
        "number_to_words_udf = F.udf(number_to_words_py, StringType())\n",
        "purchase_sum_df = vendor_df.join(purchase_df, \"vid\").groupBy(\"vadd\").agg(F.sum(\"total\").alias(\"total_amount\"))\n",
        "purchase_sum_df.withColumn(\"amount_in_words\", number_to_words_udf(F.col(\"total_amount\"))).show(truncate=False)\n",
        "# -- Spark SQL Solution --\n",
        "spark.udf.register(\"to_words\", number_to_words_py, StringType())\n",
        "spark.sql(\"\"\"\n",
        "    SELECT vadd, total_amount, to_words(total_amount) as amount_in_words FROM (\n",
        "        SELECT v.vadd, SUM(p.total) AS total_amount\n",
        "        FROM vendor_table v\n",
        "        JOIN purchase_table p ON v.vid = p.vid\n",
        "        GROUP BY v.vadd\n",
        "    )\n",
        "\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b90afdc",
      "metadata": {
        "id": "5b90afdc"
      },
      "source": [
        "## <a id='q49'></a>Q.49 Has the customer who has spent the largest amount money been given the highest rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b21c8d",
      "metadata": {
        "id": "e3b21c8d"
      },
      "outputs": [],
      "source": [
        "# Q.49 Has the customer who has spent the largest amount money been given the highest rating.\n",
        "# -- DataFrame API Solution --\n",
        "spending_df = ser_det_df.groupBy(\"cid\").agg(F.sum(\"total\").alias(\"total_spent\"))\n",
        "window_spec = Window.orderBy(F.col(\"total_spent\").desc())\n",
        "rating_df = spending_df.withColumn(\"rank\", F.dense_rank().over(window_spec)).withColumn(\"rating\", F.when(F.col(\"rank\") == 1, 'A').when(F.col(\"rank\") <= 3, 'B').otherwise('C'))\n",
        "customer_df.join(rating_df, \"cid\").orderBy(F.col(\"total_spent\").desc()).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    WITH CustomerSpending AS (\n",
        "        SELECT cid, SUM(total) as total_spent\n",
        "        FROM ser_det_table\n",
        "        GROUP BY cid\n",
        "    ),\n",
        "    CustomerRating AS (\n",
        "        SELECT cid, total_spent,\n",
        "            CASE\n",
        "                WHEN DENSE_RANK() OVER (ORDER BY total_spent DESC) = 1 THEN 'A'\n",
        "                WHEN DENSE_RANK() OVER (ORDER BY total_spent DESC) <= 3 THEN 'B'\n",
        "                ELSE 'C'\n",
        "            END as rating\n",
        "        FROM CustomerSpending\n",
        "    )\n",
        "    SELECT c.cname, cr.total_spent, cr.rating\n",
        "    FROM CustomerRating cr\n",
        "    JOIN customer_table c ON cr.cid = c.cid\n",
        "    ORDER BY cr.total_spent DESC\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e10075",
      "metadata": {
        "id": "c1e10075"
      },
      "source": [
        "## <a id='q50'></a>Q.50 select the total amount in service for each customer for which the total is greater than the amount of the largest service amount in the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808b0ac5",
      "metadata": {
        "id": "808b0ac5"
      },
      "outputs": [],
      "source": [
        "# Q.50 select the total amount in service for each customer for which the total is greater than the amount of the largest service amount in the table.\n",
        "# -- DataFrame API Solution --\n",
        "largest_service_amount = ser_det_df.agg(F.max(\"total\")).first()[0]\n",
        "customer_total_spend_df = ser_det_df.groupBy(\"cid\").agg(F.sum(\"total\").alias(\"customer_total_spend\"))\n",
        "customer_total_spend_df.where(F.col(\"customer_total_spend\") > largest_service_amount).join(customer_df, \"cid\").select(\"cname\", \"customer_total_spend\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname, SUM(s.total) as customer_total_service_amount\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    GROUP BY c.cname\n",
        "    HAVING SUM(s.total) > (SELECT MAX(total) FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b403c6",
      "metadata": {
        "id": "84b403c6"
      },
      "source": [
        "## <a id='q51'></a>Q.51 List the customer name and sparepart name used for their vehicle and vehicle type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b977390",
      "metadata": {
        "id": "4b977390"
      },
      "outputs": [],
      "source": [
        "# Q.51 List the customer name and sparepart name used for their vehicle and vehicle type.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.join(customer_df, \"cid\").join(sparepart_df, \"spid\").select(\"cname\", \"spname\", \"type_veh\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname, sp.spname, s.type_veh\n",
        "    FROM ser_det_table s\n",
        "    JOIN customer_table c ON s.cid = c.cid\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d32c110",
      "metadata": {
        "id": "4d32c110"
      },
      "source": [
        "## <a id='q52'></a>Q.52 Get spname, ename, cname, quantity, rate, service amount for records in service table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f20bb5b",
      "metadata": {
        "id": "5f20bb5b"
      },
      "outputs": [],
      "source": [
        "# Q.52 Get spname, ename, cname, quantity, rate, service amount for records in service table.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.join(sparepart_df, \"spid\").join(employee_df, \"eid\").join(customer_df, \"cid\").select(\"spname\", \"ename\", \"cname\", \"qty\", \"sp_rate\", \"ser_amt\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT sp.spname, e.ename, c.cname, s.qty, s.sp_rate, s.ser_amt\n",
        "    FROM ser_det_table s\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "    JOIN employee_table e ON s.eid = e.eid\n",
        "    JOIN customer_table c ON s.cid = c.cid\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdda143c",
      "metadata": {
        "id": "cdda143c"
      },
      "source": [
        "## <a id='q53'></a>Q.53 specify the vehicles owners who’s tube damaged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61332c7",
      "metadata": {
        "id": "d61332c7"
      },
      "outputs": [],
      "source": [
        "# Q.53 specify the vehicles owners who’s tube damaged.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.where(F.col(\"typ_ser\") == 'TUBE DAMAGED').join(customer_df, \"cid\").select(\"cname\").distinct().show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT DISTINCT c.cname\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE s.typ_ser = 'TUBE DAMAGED'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf5b2eb",
      "metadata": {
        "id": "5bf5b2eb"
      },
      "source": [
        "## <a id='q54'></a>Q.54 Specify the details who have taken full service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad844f07",
      "metadata": {
        "id": "ad844f07"
      },
      "outputs": [],
      "source": [
        "# Q.54 Specify the details who have taken full service.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.where(F.col(\"typ_ser\") == 'FULL SERVICING').join(customer_df, \"cid\").select(\"cname\", \"cadd\", \"veh_no\", \"ser_date\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT c.cname, c.cadd, s.veh_no, s.ser_date\n",
        "    FROM customer_table c\n",
        "    JOIN ser_det_table s ON c.cid = s.cid\n",
        "    WHERE s.typ_ser = 'FULL SERVICING'\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75accc4",
      "metadata": {
        "id": "a75accc4"
      },
      "source": [
        "## <a id='q55'></a>Q.55 Select the employees who have not worked yet and left the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a92177",
      "metadata": {
        "id": "54a92177"
      },
      "outputs": [],
      "source": [
        "# Q.55 Select the employees who have not worked yet and left the job.\n",
        "# -- DataFrame API Solution --\n",
        "serviced_employees = ser_det_df.select(\"eid\").distinct()\n",
        "employee_df.where(F.col(\"edol\").isNotNull()).join(serviced_employees, \"eid\", \"left_anti\").select(\"ename\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename\n",
        "    FROM employee_table e\n",
        "    WHERE e.edol IS NOT NULL AND e.eid NOT IN (SELECT eid FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4706763",
      "metadata": {
        "id": "f4706763"
      },
      "source": [
        "## <a id='q56'></a>Q.56 Select employee who have worked first ever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0825f1",
      "metadata": {
        "id": "9a0825f1"
      },
      "outputs": [],
      "source": [
        "# Q.56 Select employee who have worked first ever.\n",
        "# -- DataFrame API Solution --\n",
        "first_service_date = ser_det_df.select(F.min(\"ser_date\")).first()[0]\n",
        "ser_det_df.where(F.col(\"ser_date\") == first_service_date).join(employee_df, \"eid\").select(\"ename\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.ename\n",
        "    FROM employee_table e\n",
        "    JOIN ser_det_table s ON e.eid = s.eid\n",
        "    WHERE s.ser_date = (SELECT MIN(ser_date) FROM ser_det_table)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91b7821",
      "metadata": {
        "id": "d91b7821"
      },
      "source": [
        "## <a id='q57'></a>Q.57 Display all records falling in odd date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163bf959",
      "metadata": {
        "id": "163bf959"
      },
      "outputs": [],
      "source": [
        "# Q.57 Display all records falling in odd date.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.where(F.dayofmonth(F.to_date(F.col(\"ser_date\"), \"dd-MMM-yy\")) % 2 != 0).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT * FROM ser_det_table WHERE MOD(DAY(TO_DATE(ser_date, 'dd-MMM-yy')), 2) != 0\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eecadaf",
      "metadata": {
        "id": "8eecadaf"
      },
      "source": [
        "## <a id='q58'></a>Q.58 Display all records falling in even date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3bcaf2",
      "metadata": {
        "id": "7e3bcaf2"
      },
      "outputs": [],
      "source": [
        "# Q.58 Display all records falling in even date.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.where(F.dayofmonth(F.to_date(F.col(\"ser_date\"), \"dd-MMM-yy\")) % 2 == 0).show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"SELECT * FROM ser_det_table WHERE MOD(DAY(TO_DATE(ser_date, 'dd-MMM-yy')), 2) = 0\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6186791",
      "metadata": {
        "id": "e6186791"
      },
      "source": [
        "## <a id='q59'></a>Q.59 Display the vendors whose material is not yet used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dbfc0b",
      "metadata": {
        "id": "32dbfc0b"
      },
      "outputs": [],
      "source": [
        "# Q.59 Display the vendors whose material is not yet used.\n",
        "# -- DataFrame API Solution --\n",
        "used_spids = ser_det_df.select(\"spid\").distinct()\n",
        "purchased_but_unused_spids = purchase_df.join(used_spids, \"spid\", \"left_anti\").select(\"vid\").distinct()\n",
        "purchased_but_unused_spids.join(vendor_df, \"vid\").select(\"vname\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT v.vname\n",
        "    FROM vendor_table v\n",
        "    WHERE v.vid IN (\n",
        "        SELECT p.vid FROM purchase_table p\n",
        "        WHERE p.spid NOT IN (SELECT spid FROM ser_det_table)\n",
        "    )\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7938bc7",
      "metadata": {
        "id": "b7938bc7"
      },
      "source": [
        "## <a id='q60'></a>Q.60 Difference between purchase date and used date of spare part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e934e7f4",
      "metadata": {
        "id": "e934e7f4"
      },
      "outputs": [],
      "source": [
        "# Q.60 Difference between purchase date and used date of spare part.\n",
        "# -- DataFrame API Solution --\n",
        "ser_det_df.join(purchase_df, \"spid\").join(sparepart_df, \"spid\").withColumn(\"days_difference\", F.datediff(F.to_date(F.col(\"ser_date\"), \"dd-MMM-yy\"), F.to_date(F.col(\"pdate\"), \"dd-MMM-yy\"))).select(\"sid\", \"spname\", \"pdate\", \"ser_date\", \"days_difference\").show()\n",
        "# -- Spark SQL Solution --\n",
        "spark.sql(\"\"\"\n",
        "    SELECT s.sid, sp.spname, p.pdate, s.ser_date,\n",
        "           DATEDIFF(TO_DATE(s.ser_date, 'dd-MMM-yy'), TO_DATE(p.pdate, 'dd-MMM-yy')) AS days_difference\n",
        "    FROM ser_det_table s\n",
        "    JOIN purchase_table p ON s.spid = p.spid\n",
        "    JOIN sparepart_table sp ON s.spid = sp.spid\n",
        "\"\"\").show()\n",
        "\n",
        "# Stop the Spark Session (uncomment to stop at the end)\n",
        "# spark.stop()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}